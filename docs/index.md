# Neural Network Mathematical Theory
As part of my bachelor's in mathematics, I had the fortune of discovering the concept of Neural Networks; these are mathematical structures that try to replicate the learning process to solve problems like Regression, Classification, and Clustering. This concept involved fields like Analysis, Statistics, Linear Algebra, etc. Additionally, the knowledge of programming in languages like Python or R.

As a purist mathematician, I already had experience in these fields, but the process of understanding how exactly neural networks work, requires time, curiosity, and perseverance. I intend to share my interpretation and finds in the literature.

## Introduction to Neural Network Structure
One initial approach to this concept is the "Universal Approximation Theorem" proposed by George Cybenko in 1989. This theorem proves that the Multilayer Perceptron approaches any continuous function as much as we want without ensuring complete convergence.

In this first document, we answer the question Â¿Why does multilayer perceptron have this structure? Exploring the mathematical concept of itself and the significates of activation functions.

<a href="/pdfs/ProjMarr_slides.pdf" class="image fit"><img src="images/marr_pic.jpg" alt=""></a>
